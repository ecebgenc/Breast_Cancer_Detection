# -*- coding: utf-8 -*-
"""Breast_Cancer_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q_B1E8m0YepmebjB_AR0Xldp1FktZTPy
"""

# Import Libraries
import itertools
import warnings
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
import math
from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
import warnings
warnings.filterwarnings('ignore')

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_validate, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
!pip install catboost
from catboost import CatBoostClassifier

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.float_format', lambda x: '%.3f' % x)
pd.set_option('display.width', 500)

# Loading the datasets
train_df = pd.read_csv("/content/train.csv")
test_df = pd.read_csv("/content/test.csv")
submission_df = pd.read_csv("/content/sample_submission.csv")

# Exploring The Dataset
df = train_df.copy()
df

print(f'The shape of train data is: \n', df.shape)

print(f'The shape of test data is: \n', test_df.shape)

print(f'Data types of train data is: \n', df.dtypes)

df.head()

df.tail()

df.info()

df.describe().T

na_count = df.isnull().sum().sort_values(ascending=True)
na_count

na_ratio = (df.isnull().sum().sort_values() / len(df)) * 100
na_ratio

missing_val = pd.concat([na_count, np.round(na_ratio, 2)], axis=1, keys=['na_count', 'na_ratio'])
print(missing_val, end="\n")

import missingno as msno
msno.matrix(df)
plt.show()

test_df.isnull().sum()

msno.matrix(test_df)
plt.show()

df.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T

# df.drop("id", axis=1, inplace=True)

num_cols = [col for col in df.columns if df[col].dtype != "O" and col not in "id"]
num_cols

cat_cols = [col for col in df.columns if df[col].dtypes == "O"]
cat_cols

def summary_cat_cols(dataframe, col_name, plot=False):
    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),
                        "Ratio": 100 * dataframe[col_name].value_counts() / len(dataframe)}))
    print("##########################################")
    if plot:
        sns.countplot(x=dataframe[col_name], data=dataframe, palette="hls")
        plt.show(block=True)

for col in cat_cols:
    summary_cat_cols(df, col, plot=True)

Vals = list(pd.value_counts(df["diagnosis"]))
A = df["diagnosis"].value_counts().index.tolist()
explode = [0.1] * len(A)
plt.figure(figsize=(10, 10))
plt.title("Pie Chart of Diagnosis")
plt.pie(Vals, explode=explode, labels=["Benign", "Malignant"], autopct='%.1f%%', colors=['#B7C3F3', '#8EB897'])
plt.show()

def num_summary(dataframe, numerical_col, plot=False):
    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]
    print(dataframe[numerical_col].describe(quantiles).T)

    if plot:
        dataframe[numerical_col].hist(bins=20)
        plt.xlabel(numerical_col)
        plt.title(numerical_col)
        plt.show(block=True)

for col in num_cols:
    num_summary(df, col)

def correlation_matrix(df, cols):
    fig = plt.gcf()
    fig.set_size_inches(30, 18)
    plt.xticks(fontsize=10)
    plt.yticks(fontsize=10)
    fig = sns.heatmap(df[cols].corr(), annot=True, linewidths=0.5, annot_kws={'size': 12}, linecolor='w', cmap='Spectral')
    plt.show(block=True)

correlation_matrix(df, num_cols)

skewness = df.skew().sort_values(ascending= False)
skewness

skew_cols = ["concavity_se", "area_se","fractal_dimension_se", "perimeter_se", 
             "radius_se", "smoothness_se", "symmetry_se", "compactness_se", 
             "texture_se", "fractal_dimension_worst", "concave points_se", "area_worst",
             "area_mean", "compactness_worst", "concavity_mean", "fractal_dimension_mean",
             "compactness_mean", "concave points_mean"]
df[skew_cols].apply(np.log1p)

test_df[skew_cols].apply(np.log1p)

df.corr()

sns.clustermap(df.corr(), annot = True, fmt = ".2f", figsize=(17, 12),
    row_cluster=False,
    dendrogram_ratio=(.1, .2),
    cbar_pos=(0, .2, .03, .4),
    cmap="Spectral")
plt.title("Correlation Matrix")
plt.show()

for i in num_cols:
  sns.boxenplot(data=df, y="diagnosis", x=df[i], palette="hls")
  plt.show()

for i in num_cols:
  plt.figure(figsize=(18, 8))
  sns.histplot(data=df, x=df[i], hue="diagnosis", kde=True, palette="hls")
  plt.title("Distribution")

df.columns

mean_values = df.iloc[:, 1:11]
se_values = df.iloc[:, 11:21]
worst_values = df.iloc[:, 21:]

mean_values.hist(bins=10, grid=False, figsize=(15, 10), color="#8EB897", edgecolor="black")

se_values.hist(bins=10, grid=False, figsize=(15, 10), color="#8EB897")

worst_values.hist(bins=10, grid=False, figsize=(15, 10), color="#8EB897")

sns.pairplot(mean_values, kind="reg")
plt.show()

sns.pairplot(se_values, kind="reg")
plt.show()

sns.pairplot(worst_values, kind="reg")
plt.show()

# Data PreProcessing
# No missing value to dealing with
# Dealing with Outliers
def outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):
    quartile1 = dataframe[col_name].quantile(q1)
    quartile3 = dataframe[col_name].quantile(q3)
    interquantile_range = quartile3 - quartile1
    up_limit = quartile3 + 1.5 * interquantile_range
    low_limit = quartile1 - 1.5 * interquantile_range
    return low_limit, up_limit

def check_outlier(dataframe, col_name):
    low_limit, up_limit = outlier_thresholds(dataframe, col_name)
    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):
        return True
    else:
        return False

for col in num_cols:
    print(col, check_outlier(df, col))

def grab_outliers(dataframe, col_name, index=False):
    low, up = outlier_thresholds(dataframe, col_name)

    if dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0] > 10:
        print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].head())
    else:
        print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))])

    if index:
        outlier_index = dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].index
        return outlier_index

for col in num_cols:
    print(col, grab_outliers(df, col))

# Local Outlier Factor
from sklearn.neighbors import LocalOutlierFactor
clf = LocalOutlierFactor(n_neighbors=20)
clf.fit_predict(df.drop(["diagnosis", "id"], axis=1))

df_scores = clf.negative_outlier_factor_
df_scores[0:5]

scores = pd.DataFrame(np.sort(df_scores))
scores.plot(stacked=True, xlim=[0, 50], style='.-')
plt.show()

th = np.sort(df_scores)[6]
th

df[df_scores < th]

df[df_scores < th].shape

df[df_scores < th].index

df[df_scores < th].drop(axis=0, labels=df[df_scores < th].index)
df.head()

clf_test = LocalOutlierFactor(n_neighbors=20)
clf_test.fit_predict(test_df.drop(["id"], axis=1))

df_test_scores = clf_test.negative_outlier_factor_
df_test_scores[0:5]

test_df[df_test_scores < th].drop(axis=0, labels=test_df[df_test_scores < th].index)
test_df.head()

# Label Encoding
from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()
df["diagnosis"] = labelencoder.fit_transform(df["diagnosis"])

# Data Normalization
scaler = StandardScaler().fit_transform(df[num_cols])
df[num_cols] = pd.DataFrame(scaler, columns=df[num_cols].columns)

test_df.columns

test_cols = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 
             'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 
             'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 
             'area_se', 'smoothness_se', 'compactness_se', 'concavity_se',
             'concave points_se', 'symmetry_se', 'fractal_dimension_se', 
             'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 
             'smoothness_worst', 'compactness_worst', 'concavity_worst',
       'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']
test_data = test_df[test_cols]
test_scaled = StandardScaler().fit_transform(test_data)
test_data = pd.DataFrame(test_scaled, columns=test_data.columns)

y = df["diagnosis"]
x = df.drop(["diagnosis", "id"], axis=1)

# Model
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,
                                                    y,
                                                    stratify=y,
                                                    random_state = 152,
                                                    test_size = 0.2, 
                                                    shuffle = True)

print(f"The shape of x_train is {x_train.shape}")
print(f"The shape of x_test is {x_test.shape}")
print(f"The shape of y_train is {y_train.shape}")
print(f"The shape of y_test is {y_test.shape}")

# LR
classifier = LogisticRegression()
classifier.fit(x_train, y_train)
y_pred = classifier.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, classifier.predict_proba(x_test)[:,1])
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
metrics = pd.DataFrame({'Metrics': ['Accuracy',
                                          'ROC-AUC',
                                          'Precision',
                                          'Recall'],
                        'Scores': [accuracy, roc_auc, precision, recall]})

metrics

from sklearn.metrics import confusion_matrix
import seaborn as sns
cm = confusion_matrix(y_test, y_pred)
print(cm)
sns.heatmap(cm, annot=True, cmap="crest")

# KNeighborsClassifier
classifier = KNeighborsClassifier()
classifier.fit(x_train, y_train)
y_pred = classifier.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, classifier.predict_proba(x_test)[:,1])
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
metrics = pd.DataFrame({'Metrics': ['Accuracy',
                                          'ROC-AUC',
                                          'Precision',
                                          'Recall'],
                        'Scores': [accuracy, roc_auc, precision, recall]})
print(metrics)

from sklearn.metrics import confusion_matrix
import seaborn as sns
cm = confusion_matrix(y_test, y_pred)
print(cm)
sns.heatmap(cm, annot=True, cmap="crest")

# DecisionTreeClassifier
classifier = DecisionTreeClassifier()
classifier.fit(x_train, y_train)
y_pred = classifier.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, classifier.predict_proba(x_test)[:,1])
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
metrics = pd.DataFrame({'Metrics': ['Accuracy',
                                          'ROC-AUC',
                                          'Precision',
                                          'Recall'],
                        'Scores': [accuracy, roc_auc, precision, recall]})
print(metrics)

from sklearn.metrics import confusion_matrix
import seaborn as sns
cm = confusion_matrix(y_test, y_pred)
print(cm)
sns.heatmap(cm, annot=True, cmap="crest")

# RandomForestClassifier()
classifier = RandomForestClassifier()
classifier.fit(x_train, y_train)
y_pred = classifier.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, classifier.predict_proba(x_test)[:,1])
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
metrics = pd.DataFrame({'Metrics': ['Accuracy',
                                          'ROC-AUC',
                                          'Precision',
                                          'Recall'],
                        'Scores': [accuracy, roc_auc, precision, recall]})
print(metrics)

from sklearn.metrics import confusion_matrix
import seaborn as sns
cm = confusion_matrix(y_test, y_pred)
print(cm)
sns.heatmap(cm, annot=True, cmap="crest")

# AdaBoostClassifier
classifier = DecisionTreeClassifier()
classifier.fit(x_train, y_train)
y_pred = classifier.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, classifier.predict_proba(x_test)[:,1])
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
metrics = pd.DataFrame({'Metrics': ['Accuracy',
                                          'ROC-AUC',
                                          'Precision',
                                          'Recall'],
                        'Scores': [accuracy, roc_auc, precision, recall]})
print(metrics)

from sklearn.metrics import confusion_matrix
import seaborn as sns
cm = confusion_matrix(y_test, y_pred)
print(cm)
sns.heatmap(cm, annot=True, cmap="crest")

# GradientBoostingClassifier
classifier = GradientBoostingClassifier()
classifier.fit(x_train, y_train)
y_pred = classifier.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, classifier.predict_proba(x_test)[:,1])
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
metrics = pd.DataFrame({'Metrics': ['Accuracy',
                                          'ROC-AUC',
                                          'Precision',
                                          'Recall'],
                        'Scores': [accuracy, roc_auc, precision, recall]})
print(metrics)

from sklearn.metrics import confusion_matrix
import seaborn as sns
cm = confusion_matrix(y_test, y_pred)
print(cm)
sns.heatmap(cm, annot=True, cmap="crest")

# XGBClassifier
classifier = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
classifier.fit(x_train, y_train)
y_pred = classifier.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, classifier.predict_proba(x_test)[:,1])
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
metrics = pd.DataFrame({'Metrics': ['Accuracy',
                                          'ROC-AUC',
                                          'Precision',
                                          'Recall'],
                        'Scores': [accuracy, roc_auc, precision, recall]})
print(metrics)

from sklearn.metrics import confusion_matrix
import seaborn as sns
cm = confusion_matrix(y_test, y_pred)
print(cm)
sns.heatmap(cm, annot=True, cmap="crest")

# LGBMClassifier
classifier = LGBMClassifier()
classifier.fit(x_train, y_train)
y_pred = classifier.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, classifier.predict_proba(x_test)[:,1])
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
metrics = pd.DataFrame({'Metrics': ['Accuracy',
                                          'ROC-AUC',
                                          'Precision',
                                          'Recall'],
                        'Scores': [accuracy, roc_auc, precision, recall]})
print(metrics)

from sklearn.metrics import confusion_matrix
import seaborn as sns
cm = confusion_matrix(y_test, y_pred)
print(cm)
sns.heatmap(cm, annot=True, cmap="crest")

# CatBoostClassifier
classifier = CatBoostClassifier(verbose=False)
classifier.fit(x_train, y_train)
y_pred = classifier.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, classifier.predict_proba(x_test)[:,1])
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
metrics = pd.DataFrame({'Metrics': ['Accuracy',
                                          'ROC-AUC',
                                          'Precision',
                                          'Recall'],
                        'Scores': [accuracy, roc_auc, precision, recall]})
print(metrics)

from sklearn.metrics import confusion_matrix
import seaborn as sns
cm = confusion_matrix(y_test, y_pred)
print(cm)
sns.heatmap(cm, annot=True, cmap="crest")

submission_df.head()

# Test Data
# XGBClassifier
classifier = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
classifier.fit(x_train, y_train)
test_predictions = classifier.predict(test_data)

test_predictions

output = pd.DataFrame({'id': test_df["id"],
                       'diagnosis': test_predictions})
output["diagnosis"] = output['diagnosis'].map({0:'B',1:'M'})
output.to_csv('submission.csv', index=False)